---
title: 'Supplementary material 1: Code to implement bias adjustments'
author: "Rob Boyd"
date: "13 March 2023"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction

In this document, I provide the code to tackle a simple biodiversity monitoring problem. The first step is to estimate mean occupancy of the plant C. vulgaris in Britain in two time-periods (1987-1999 and 2010-2019) using an unrepresentative nonprobability sample. We know the "truth" in this example, so we can assess the accuracy of various sample-based estimators. The second step is to estimate the difference between the two (i.e. the trend). Again, we know the truth, so we can assess the accuracy of the estimated trends from various estimators. Each of the estimators that we use can be viewed as an attempt to weight the sample in such a way that the distributions of "auxiliary variables" in the sample more closely resemble those in the population. We will also look at how well this has been achieved. 

## Estimating per-period mean occupancy

The first step is to load the relevant packages and data. There are N rows in the data, where N is the number of land-containing 1 km grid squares in Great Britain (minus a few for which the auxiliary data are not available). There is one column per variable. heather_true_dist_1987.1999 is the binary response variable (1 = occupied and 0 = unoccupied) for the first time-period; heather_true_dist_2010.2019 is the same but for the second time period. sampled_units_1987.1999 and sampled_units_2010.2019 are also binary and indicate whether the grid square was sampled in each period. The next five columns are auxiliary variables, which are known for every grid square. The final two columns are estimated inclusion probabilities, which were derived using random forests and the auxiliary data.
```{r warnings = FALSE}
library(raster)
library(ggplot2)
library(survey)
library(rstanarm)
library(PracTools)
library(reshape2)
library(dplyr)
library(caret)

## load population data
pop <- read.csv("W:/PYWELL_SHARED/Pywell Projects/BRC/Rob Boyd/NERC_exploring_frontiers/Data/all_data.csv")

pop <- pop[,-c(10,11,14)] # drop climate variable, which we didn't use in the end 

pop <- pop[complete.cases(pop), ]
```
Some approaches to adjusting samples, such as poststratification, require categorical axiliary variables. The auxiliary variables in pop are continuous, so they need to be discretized. We split three of the the variables into three catgories (i.e. cut points at the 33rd and 67th percentiles). This did not make sense for two variables, openAccessGB (proportion of grid square that is open access) and allPACoverage (proportion of grid square that is in a protected area), because most grid squares take the values zero or one. For these variables, we split the data into into two categories, 0 and >0, i.e. whether some of the grid square is protected or open acessess. 
```{r warnings = FALSE}
pop_disc <- pop

pop_aux_cont <- pop_disc[,5:9]

## discretize the auxiliary data in pop
for (i in c(5,8,9)) {

  q <- as.numeric(quantile(pop_disc[,i], probs = c(0, 0.33, 0.66, 1)))
  
  pop_disc[,i] <- cut(pop_disc[,i], 
                 breaks = q,
                 labels = FALSE,
                 include.lowest = TRUE,
                 right = TRUE)
  
  pop_disc[,i] <- as.numeric(pop_disc[,i])
  
}

## make PA and open access land coverage binary 

pop_disc$openAccessGB <- ifelse(pop$openAccessGB > 0, 1, 0)

pop_disc$allPACoverage <- ifelse(pop$allPACoverage > 0, 1, 0)
```
There are a few data wrangling tasks to do next. We need to subset pop to create dataframes specific to each time-period and sampled grid squares only. 
```{r warnings = FALSE}
## pull out columns relevant to period 1
pop_p1 <- pop[,c(1,3,5,6,7,8,9,10)]

pop_disc_p1 <- pop_disc[,c(1,3,5,6,7,8,9,10)]

## and period 2
pop_p2 <- pop[,c(2,4,5,6,7,8,9,11)]

pop_disc_p2 <- pop_disc[,c(2,4,5,6,7,8,9,11)]

## pull out sampled rows for periods 1 and 2
samp_disc_p1 <- pop_disc_p1[pop_disc_p1$sampled_units_1987.1999 == 1, ]

samp_disc_p2 <- pop_disc_p2[pop_disc_p2$sampled_units_2010.2019 == 1, ]

## pull out the auxiliary data for the whole population
pop_aux <- pop_disc[,5:9]
```
The first step in our simple biodiversity monitoring problem is to estimate mean occupancy in each time-period. The true means are 0.317 in period one and 0.270 in period two.
```{r warnings = FALSE}
pop_mean_p1 <- mean(pop_p1$heather_true_dist_1987.1999);pop_mean_p1

pop_mean_p2 <- mean(pop_p2$heather_true_dist_2010.2019);pop_mean_p2
```
In real life, we don't know the population means so have to estimate them. The R package survey provides functionality for estimating population parameters from samples. The first job is to create what is called a survey design object, which includes the data in the sample and information about the survey design. In our case, we don't know anything about the "survey" (or lack thereof), so the code is simple. 
```{r }
design_p1 <- svydesign(ids=~0,
                       data = samp_disc_p1)

design_p2 <- svydesign(ids=~0,
                       data = samp_disc_p2)
```
The warnings tell us that the package will assume equal sampling weights. This is not a problem, because we will adjust the weights later. Setting ids to ~0 just tells the function that we are not aware of any clustering in the data. 

Using the survey designs, we can calculate the sample means for each period and the associated confidence intervals. The sample mean will act a baseline, and the aim is to improve on it by weighting. 
```{r }
samp_mean_p1 <- svymean(design = design_p1,
                        x=~heather_true_dist_1987.1999); samp_mean_p1

samp_mean_p2 <- svymean(design = design_p2,
                        x=~heather_true_dist_2010.2019); samp_mean_p2

## and their confidence intervals
samp_mean_p1_conf <- confint(object = samp_mean_p1,
                             level = 0.95); samp_mean_p1_conf

samp_mean_p2_conf <- confint(object = samp_mean_p2,
                             level = 0.95); samp_mean_p2_conf
```
The first set of weights we will try are the inverses of inclusion probabilities that were estimated using random forests. This approach is called quasi-randomisation.
```{r }
weighted_design_p1 <- svydesign(ids=~0,
                                data = samp_disc_p1,
                                probs=~inclusionProbs_1987.1999)

weighted_design_p2 <- svydesign(ids=~0,
                                data = samp_disc_p2,
                                probs=~inclusionProbs_2010.2019)

## then get the weighted sample means
weighted_samp_mean_p1 <- svymean(design = weighted_design_p1,
                                 x=~heather_true_dist_1987.1999);weighted_samp_mean_p1

weighted_samp_mean_p2 <- svymean(design = weighted_design_p2,
                                 x=~heather_true_dist_2010.2019);weighted_samp_mean_p2

## and their confidence intervals 
weighted_samp_mean_p1_conf <- confint(object = weighted_samp_mean_p1,
                                      level = 0.95);weighted_samp_mean_p1_conf

weighted_samp_mean_p2_conf <- confint(object = weighted_samp_mean_p2,
                                      level = 0.95);weighted_samp_mean_p2_conf
```
Next we can try poststratification.
```{r }
cells <- data.frame(table(pop_aux))

str(cells)

## now poststratify 
ps_design_p1 <- postStratify(design = design_p1,
                             strata = samp_disc_p1[,3:7],
                             population = data.frame(table(pop_aux)),
                             partial = T)

ps_design_p2 <- postStratify(design = design_p2,
                             strata = samp_disc_p2[,3:7],
                             population = data.frame(table(pop_aux)),
                             partial = T)

## and get the weighted mean across poststrata
ps_samp_mean_p1 <- svymean(design = ps_design_p1,
                           x=~heather_true_dist_1987.1999,
                           na.rm = T);ps_samp_mean_p1

ps_samp_mean_p2 <- svymean(design = ps_design_p2,
                                 x=~heather_true_dist_2010.2019);ps_samp_mean_p2


## and their confidence intervals
ps_samp_mean_p1_conf <- confint(object = ps_samp_mean_p1,
                                level = 0.95);ps_samp_mean_p1_conf

ps_samp_mean_p2_conf <- confint(object = ps_samp_mean_p2,
                                level = 0.95);ps_samp_mean_p2_conf
```
It is instructive to look at the variable of interest in each category of the auxiliary variables (recalling that we discretized them earlier). It is good to see that mean occupancy varies among categories, because this implies that there is something to be gained by poststratifying. If there was little difference, the adjustment from poststratifying would be minor.
```{r }
s1 <- samp_disc_p1[,-8] # drop estimated inclusion probabilities, which aren't needed here

colnames(s1) <- c("y", "R", "postcode_density", "open_access", "protected_area",
                       "road_length", "elevation")

s1$period <- "Period_1"

s2 <- samp_disc_p2[,-8]

colnames(s2) <- c("y", "R", "postcode_density", "open_access", "protected_area",
                  "road_length", "elevation")

s2$period <- "Period_2"

s <- rbind(s1, s2)

s <- melt(s, id = c("y", "period", "R"))

ggplot(data = s, aes(x = factor(value), y = y)) +
  geom_bar(stat = "summary", fun = "mean") +
  facet_grid(period~variable, scales = "free") +
  theme_linedraw() +
  labs(x = "Level", y = "Mean occupancy")

```

Another approach to weighting the sample mean is superpopulation modelling, which can be implemented using the function calibrate. It is better to use the continuous rather than discretized auxiliary variables for this approach, so we begin by creating two new survey designs based on those data (i.e. pop not pop_disc). The calibrate function also requires population totals for each auxiliary variable. The intercept total is the population size and must be named "(Intercept)".
```{r }
## get population totals for auxiliary variables
aux_tots <- c(nrow(pop_aux_cont), colSums(pop_aux_cont))

names(aux_tots)[1] <- "(Intercept)"

## create new designs with the continuous rather than discretized auxiliary variables
samp_p1 <- pop_p1[pop_p1$sampled_units_1987.1999 == 1, ]

samp_p2 <- pop_p2[pop_p2$sampled_units_2010.2019 == 1, ]

pre_calib_design_p1 <- svydesign(ids=~0,
                       data = samp_p1)

pre_calib_design_p2 <- svydesign(ids=~0,
                       data = samp_p2)

## now calibrate 
calib_design_p1 <- calibrate(design = pre_calib_design_p1,
                             formula = ~ postcode_density_299_neighbours +
                               openAccessGB + allPACoverage + 
                               road_length_299_neighbours + UKelv,
                             population = aux_tots,
                             calfun="linear")

sum(weights(calib_design_p1)) # check weights sum to the population size

summary(weights(calib_design_p1))

sp_samp_mean_p1 <- svymean(~heather_true_dist_1987.1999, design=calib_design_p1); sp_samp_mean_p1

calib_design_p2 <- calibrate(design = pre_calib_design_p2,
                             formula = ~ postcode_density_299_neighbours +
                               openAccessGB + allPACoverage + 
                               road_length_299_neighbours + UKelv,
                             population = aux_tots,
                             calfun="linear")

sum(weights(calib_design_p2))

summary(weights(calib_design_p2))

sp_samp_mean_p2 <- svymean(~heather_true_dist_2010.2019, design=calib_design_p2); sp_samp_mean_p2

## and the confidence intervals 
sp_samp_mean_p1_conf <- confint(object = sp_samp_mean_p1,
                                level = 0.95); sp_samp_mean_p1_conf

sp_samp_mean_p2_conf <- confint(object = sp_samp_mean_p2,
                                level = 0.95); sp_samp_mean_p2_conf

```
Perhaps more familiar to ecologists than the above approaches is subsampling. Our approach was to draw weighted random samples of size 500 with replacement from the original samples [note that these are different to sampling weights in 2)]. Using the postrata created above, I assign each grid square a weight equal to the proportion of the population in its stratum. The result is a subsample whose members were more likely to be from strata comprising a larger fraction of the population. The subsample mean is the estimator of the population mean. Note that I bootstrap the procedure. This is necessary because there is a random element to the downsampling, and the estimated means are sensitive to it. 

```{r}
## concatenate the levels of the variables in each column of sub_samp_disc to store unique strata as "id"
samp_p1$id <- paste0(samp_disc_p1$postcode_density_299_neighbours,
                              samp_disc_p1$openAccessGB,
                              samp_disc_p1$allPACoverage,
                              samp_disc_p1$road_length_299_neighbours,
                              samp_disc_p1$UKelv)

pop$id <- paste0(pop_disc$postcode_density_299_neighbours,
                         pop_disc$openAccessGB,
                         pop_disc$allPACoverage,
                         pop_disc$road_length_299_neighbours,
                         pop_disc$UKelv)

pop_props <- data.frame(table(pop$id))

pop_props$prop <- pop_props$Freq/sum(pop_props$Freq)

colnames(pop_props)[1] <- "id"

samp_props_p1 <- merge(pop_props, samp_p1, by = "id")

sub_samp_means_p1 <- lapply(1:1000,
       function(x) {
         subSamp <- samp_props_p1[sample(1:nrow(samp_props_p1), 
                                         size = 200, 
                                         replace = T, 
                                         prob = samp_props_p1$prop),]
         
         data.frame(mean = mean(subSamp$heather_true_dist_1987.1999),
                                id = x)
       })

sub_samp_means_p1 <- do.call("rbind", sub_samp_means_p1)

sub_samp_p1_mean <- c(mean = mean(sub_samp_means_p1$mean),
                      lower = quantile(sub_samp_means_p1$mean, probs = 0.025),
                      upper = quantile(sub_samp_means_p1$mean, probs = 0.975)); sub_samp_p1_mean

## and period 2 

samp_p2$id <- paste0(samp_disc_p2$postcode_density_299_neighbours,
                     samp_disc_p2$openAccessGB,
                     samp_disc_p2$allPACoverage,
                     samp_disc_p2$road_length_299_neighbours,
                     samp_disc_p2$UKelv)

head(samp_p2)

samp_props_p2 <- merge(pop_props, samp_p2, by = "id")

sub_samp_means_p2 <- lapply(1:1000,
                            function(x) {
                              subSamp <- samp_props_p2[sample(1:nrow(samp_props_p2), 
                                                              size = 200, 
                                                              replace = T, 
                                                              prob = samp_props_p2$prop),]
                              
                              data.frame(mean = mean(subSamp$heather_true_dist_2010.2019),
                                         id = x)
                            })

sub_samp_means_p2 <- do.call("rbind", sub_samp_means_p2)

sub_samp_p2_mean <- c(mean = mean(sub_samp_means_p2$mean),
                      lower = quantile(sub_samp_means_p2$mean, probs = 0.025),
                      upper = quantile(sub_samp_means_p2$mean, probs = 0.975)); sub_samp_p2_mean
```
The decision to create subsamples of size 500 was somewhat arbitrary. Changing the size of the subsamples makes little difference to their point estimates, but, naturally, the width of the confidence intervals decrease with increasing sample size. The code below demonstrates this for period one, but the result is the same for period two.
```{r}
getSens <- function(n) {
  
  sub_samp_means_p1 <- lapply(1:1000,
                              function(x) {
                                subSamp <- samp_props_p1[sample(1:nrow(samp_props_p1), 
                                                                size = n, 
                                                                replace = T, 
                                                                prob = samp_props_p1$prop),]
                                
                                data.frame(mean = mean(subSamp$heather_true_dist_1987.1999),
                                           id = x)
                              })
  
  sub_samp_means_p1 <- do.call("rbind", sub_samp_means_p1)
  
  sub_samp_p1_mean <- data.frame(mean = mean(sub_samp_means_p1$mean),
                        lower = quantile(sub_samp_means_p1$mean, probs = 0.025),
                        upper = quantile(sub_samp_means_p1$mean, probs = 0.975),
                        n = n)
  
}

sens <- lapply(c(100,200,400,800,1600,3200),
               getSens)

sens <- do.call("rbind", sens)

ggplot(data = sens, aes(x = n, y = mean)) +
  geom_ribbon(aes(ymin =lower, ymax = upper), fill = "grey", alpha = 0.3) +
  geom_line() +
  theme_linedraw() + 
  geom_point() 
```

The next approach to estimating the populations means is Multilevel Regression and Poststratification (MRP). It involes constructing a hierarchical model predicting mean occupancy in each poststratum (same strata as earlier) based on the auxiliaries. Including interactions is beneficial if there is evidence for them. The following code, which is not actually evaluated here, demonstrated that interactions are likely among all auxiliaries. 
```{r eval=FALSE}
## period one 

for (i in 3:7) {
  
  for (j in 3:7) {
    
    print(
      summary(aov(samp_disc_p1$heather_true_dist_1987.1999 ~ samp_disc_p1[,i] * samp_disc_p1[,j]))
    )
    
    interaction.plot(x.factor = samp_disc_p1[,i],
                     trace.factor = samp_disc_p1[,j],
                     response = samp_disc_p1$heather_true_dist_1987.1999,
                     fun = mean,
                     xlab = colnames(samp_disc_p1)[i],
                     ylab = "mean occupancy",
                     trace.label = colnames(samp_disc_p1)[j])
    
    Sys.sleep(3)
    
  }
  
  
}

## period two 

for (i in 3:7) {
  
  for (j in 3:7) {
    
    print(
      summary(aov(samp_disc_p2$heather_true_dist_2010.2019 ~ samp_disc_p2[,i] * samp_disc_p2[,j]))
    )
    
    interaction.plot(x.factor = samp_disc_p2[,i],
                     trace.factor = samp_disc_p2[,j],
                     response = samp_disc_p2$heather_true_dist_2010.2019,
                     fun = mean,
                     xlab = colnames(samp_disc_p2)[i],
                     ylab = "mean occupancy",
                     trace.label = colnames(samp_disc_p2)[j])
    
    Sys.sleep(3)
    
  }

  
}
```
Strictly speaking, MRP does not produce unit level weights (i.e. a weight for every grid square), but the concept is very similar. Another feature of MRP is that it is computationally demanding.
```{r eval=FALSE}
## fit model
fit <- stan_glmer(
 heather_true_dist_1987.1999 ~ 1 + (1 | postcode_density_299_neighbours) + 
    (1 | road_length_299_neighbours) + (1 | allPACoverage) + 
    (1 | openAccessGB) + (1 | UKelv),
  family = binomial(link = "logit"),
  data = samp_disc_p1,
  chains = 3,
  iter = 5000
)

print(fit)

posterior_prob <- posterior_linpred(fit, transform = T, newdata = cells)

poststrat_prob <- posterior_prob %*% cells$Freq / sum(cells$Freq)

props_p1 <- samp_means_per_cell_p1$n / samp_means_per_cell_p1$N

props_p1[is.na(props_p1)] <- 0

poststrat_prob <- posterior_prob %*% props_p1
  
#write.csv(poststrat_prob,
#          "W:/PYWELL_SHARED/Pywell Projects/BRC/Rob Boyd/NERC_exploring_frontiers/Data/poststrat_prob.csv",
#         row.names = F)

model_popn_pref <- c(mean = mean(poststrat_prob),
                     lower = quantile(poststrat_prob, probs = 0.025),
                     upper = quantile(poststrat_prob, probs = 0.975))

round(model_popn_pref, 3)

fit2 <- stan_glmer(
  heather_true_dist_2010.2019 ~ 1 + (1 | postcode_density_299_neighbours) + 
    (1 | road_length_299_neighbours) + (1 | allPACoverage) + 
    (1 | openAccessGB) + (1 | UKelv),
  family = binomial(link = "logit"),
  data = samp_disc_p2,
  chains = 2
)

print(fit2)

posterior_prob_p2 <- posterior_linpred(fit2, transform = TRUE, newdata = cells)

poststrat_prob_p2 <- posterior_prob_p2 %*% cells$Freq / sum(cells$Freq)

#write.csv(poststrat_prob_p2,
#          "W:/PYWELL_SHARED/Pywell Projects/BRC/Rob Boyd/NERC_exploring_frontiers/Data/poststrat_prob_p2.csv",
#          row.names = F)

model_popn_pref_p2 <- c(mean = mean(poststrat_prob_p2),
                        lower = quantile(poststrat_prob_p2, probs = 0.025),
                        upper = quantile(poststrat_prob_p2, probs = 0.975))

#round(model_popn_pref_p2, 3)
```
Instead we load the posterior distributions from models fitted on a computer cluster. 
```{r}
poststrat_prob <- read.csv("W:/PYWELL_SHARED/Pywell Projects/BRC/Rob Boyd/NERC_exploring_frontiers/Data/mrp_p1.csv")[,1]

model_popn_pref <- c(mean = mean(poststrat_prob),
                     lower = quantile(poststrat_prob, probs = 0.025),
                     upper = quantile(poststrat_prob, probs = 0.975))

poststrat_prob_p2 <- read.csv("W:/PYWELL_SHARED/Pywell Projects/BRC/Rob Boyd/NERC_exploring_frontiers/Data/mrp_p2.csv")[,1]

model_popn_pref_p2 <- c(mean = mean(poststrat_prob_p2),
                        lower = quantile(poststrat_prob_p2, probs = 0.025),
                        upper = quantile(poststrat_prob_p2, probs = 0.975))
```
We can combine the estimates of the population means in each time-period and plot them to get an idea of which methods work best.
```{r}
plotDat <- data.frame(p = c(1,2,1,2,1,2,1,2,1,2,1,2,1,2),
                      est = c(pop_mean_p1[1], pop_mean_p2[1], samp_mean_p1[1], samp_mean_p2[1], ps_samp_mean_p1[1], ps_samp_mean_p2[1], weighted_samp_mean_p1[1], weighted_samp_mean_p2[1], sp_samp_mean_p1[1], sp_samp_mean_p2[1], model_popn_pref[1],model_popn_pref_p2[1], sub_samp_p1_mean[1], sub_samp_p2_mean[1]),
                      type = c("Population", "Population", "Sample", "Sample", "Poststratification", "Poststratification", "Quasi-
randomisation", "Quasi-
randomisation", "Superpopulation
model", "Superpopulation
model", "MRP", "MRP", "Subsample", "Subsample"),
                      lower = c(pop_mean_p1[1], pop_mean_p2[1], samp_mean_p1_conf[1], samp_mean_p2_conf[1], ps_samp_mean_p1_conf[1], ps_samp_mean_p2_conf[1], weighted_samp_mean_p1_conf[1], weighted_samp_mean_p2_conf[1], sp_samp_mean_p1_conf[1], sp_samp_mean_p2_conf[1], model_popn_pref[2], model_popn_pref_p2[2], sub_samp_p1_mean[2], sub_samp_p2_mean[2]),
                      upper = c(pop_mean_p1[2], pop_mean_p2[2], samp_mean_p1_conf[2], samp_mean_p2_conf[2], ps_samp_mean_p1_conf[2], ps_samp_mean_p2_conf[2], weighted_samp_mean_p1_conf[2], weighted_samp_mean_p2_conf[2], sp_samp_mean_p1_conf[2], sp_samp_mean_p2_conf[2], model_popn_pref[3], model_popn_pref_p2[3], sub_samp_p1_mean[3], sub_samp_p2_mean[3]))

print(
ggplot(data = plotDat, aes(x = p, y = est, colour = type, fill = type)) +
  geom_point() +
  geom_line() +
  theme_linedraw() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.5) +
  labs(x = "",
       y = "Estimate",
       fill = "",
       colour = "") +
  scale_x_continuous(breaks = c(1,2), labels = c("1987-1999", "2010-2019"))
)
```

## Estimating the trend in mean occupancy

The second part of our simple biodiversity monitoring problem is to estimate the difference in mean occupancy between the two time-periods (i.e. the trend). It is simple to obtain point estimates of the trends from each method.
```{r}
trends <- lapply(unique(plotDat$type),
                 function(x) {
                   data.frame(difference = plotDat$est[plotDat$p == 2 & plotDat$type == x] - plotDat$est[plotDat$p == 1 & plotDat$type == x],
                              estimator = x)
                 })

trends <- do.call("rbind", trends)
```
The standard errors and confidence intervals are more complicated. The standard error of a difference in means is the square root of the sum of sampling variances of the two means. We obtained the sampling variances by squaring the standard errors provided by the survey package. MRP is different because the 95% credible interval of its trend can be extracted directly from the posterior distribution of the difference. 
```{r}
## sample mean 
sample_mean_se <- sqrt(SE(ps_samp_mean_p1)^2 + SE(ps_samp_mean_p2)^2)

sample_mean_upper <- (samp_mean_p2 - samp_mean_p1) + 1.96 * sample_mean_se

sample_mean_lower <- (samp_mean_p2 - samp_mean_p1) - 1.96 * sample_mean_se

## quasi-randomisation 
weighted_mean_se <- sqrt(SE(weighted_samp_mean_p1)^2 + SE(weighted_samp_mean_p2)^2)

weighted_mean_upper <- (weighted_samp_mean_p2 - weighted_samp_mean_p1) + 1.96 * weighted_mean_se

weighted_mean_lower <- (weighted_samp_mean_p2 - weighted_samp_mean_p1) - 1.96 * weighted_mean_se

## poststratification
ps_mean_se <- sqrt(SE(ps_samp_mean_p1)^2 + SE(ps_samp_mean_p2)^2)

ps_mean_upper <- (ps_samp_mean_p2 - ps_samp_mean_p1) + 1.96 * ps_mean_se

ps_mean_lower <- (ps_samp_mean_p2 - ps_samp_mean_p1) - 1.96 * ps_mean_se

## superpopulation
sp_mean_se <- sqrt(SE(sp_samp_mean_p1)^2 + SE(sp_samp_mean_p2)^2)

sp_mean_upper <- (sp_samp_mean_p2 - sp_samp_mean_p1) + 1.96 * sp_mean_se

sp_mean_lower <- (sp_samp_mean_p2 - sp_samp_mean_p1) - 1.96 * sp_mean_se

## subsampling

diffsSub <- sub_samp_p2_mean - sub_samp_p1_mean

#diffs <- diffs[,1]

sub_up <- quantile(diffsSub, probs = 0.975)

sub_low <- quantile(diffsSub, probs = 0.025)

## MRP 

diffs <- poststrat_prob_p2 - poststrat_prob

#diffs <- diffs[,1]

MRP_up <- quantile(diffs, probs = 0.975)

MRP_low <- quantile(diffs, probs = 0.025)

head(trends)

trends$lower <- c(NA, sample_mean_lower, ps_mean_lower, weighted_mean_lower, sp_mean_lower, MRP_low, sub_low)

trends$upper <- c(NA, sample_mean_upper, ps_mean_upper, weighted_mean_upper, sp_mean_upper, MRP_up, sub_up)

ggplot(data = trends, aes(x = difference, y = estimator)) +
         geom_point() + 
         theme_linedraw() +
         geom_vline(xintercept = 0) +
  labs(x = "Trend",
       y = "") +
  geom_errorbar(aes(xmin = lower, xmax = upper, width = .2))
```

## Visualizing the effects of weighting on the distributions of auxiliary variables

So far we have seen that weighting generally improves the accuracy of the estimates of mean occupancy in each period and the difference between the two. To see how it is doing this, it is instructive to look at the distributions of the auxiliaries in the sample, the weighted sample and the population. If the distributions in the weighted sample are closer to those in the original sample to those in the population, then weighting has been successful.

It is simple to obtain the weighted distributions for the superpopulation model, poststratification and quasi-randomisation, because we have the weights. When I implemented subsampling, however, I did not explicitly calculate weights. Instead, I wrote a simple function to extract the relative frequency distributions of the auxiliaries in the subsample.
```{r}
getRelFreqs <- function(dat, breaks, var, period, iter, bins) {
  
  sub <- lapply(1:iter,
                function(x) {
                  
                  subSamp <- dat[sample(1:nrow(dat), 
                                       size = 500, 
                                       replace = F, 
                                       prob = dat$prop),]

                  z <- data.frame(val = subSamp[,var],
                                  j = x)

                  ints <- findInterval(z$val, sort(unique(as.numeric(c(lower = as.numeric( sub("\\((.+),.*", "\\1", bins) ),
                                                                       upper = as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", bins) ))))))

                  bin_counts <- table(ints)

                  bin_counts_vec <- rep(0, 50)
                  
                  for (bin in 1:50) {
                    count <- bin_counts[as.character(bin)]
                    if (!is.na(count)) {
                      bin_counts_vec[bin] <- count
                    }
                  }

                  bin_rel_freqs <- bin_counts_vec / sum(bin_counts_vec)

                })
  
  sub <- do.call("cbind", sub)
  
  sub <- rowMeans(sub)
  
  data.frame(bin = 1:50,
             var = 1,
             id = period,
             weightType = "Subsample", 
             variable = "weighted_sample", 
             value = sub)
  
}

## period one

# elevation
relF_p1_elev <- getRelFreqs(dat = samp_props_p1,
                            breaks = 50, 
                            var = "UKelv",
                            period = "Period_1",
                            iter = 100,
                            bins = cut(pop$UKelv, breaks = 50))

# road length 
relF_p1_road <- getRelFreqs(dat = samp_props_p1,
                            breaks = 50, 
                            var = "road_length_299_neighbours",
                            period = "Period_1",
                            iter = 100,
                            bins = cut(pop$road_length_299_neighbours, breaks = 50))

## and period two 

# elevation
relF_p2_elev <- getRelFreqs(dat = samp_props_p2,
                            breaks = 50, 
                            var = "UKelv",
                            period = "Period_2",
                            iter = 100,
                            bins = cut(pop$UKelv, breaks = 50))

#road length
relF_p2_road <- getRelFreqs(dat = samp_props_p2,
                            breaks = 50, 
                            var = "road_length_299_neighbours",
                            period = "Period_2",
                            iter = 100,
                            bins = cut(pop$road_length_299_neighbours, breaks = 50))

```
I have also written a function, relFreqPlot, that uses the weights calculated earlier to produce relative frequency plots the auxiliaries in the weighted samples and compares these to the distributions in the unadjusted samples and population. The function requires weights, which we do not have for the subsampling estimator. However, it does accept relative frequencies for a specified variable, which we created for the subsampling estimator using getRelFreqs earlier.
```{r}
relFreqPlot <- function(pop,
                        R,
                        x,
                        weights,
                        breaks,
                        RNames,
                        WNames,
                        addVarByRelFreq = FALSE,
                        varByRelFreq) {

  dat <- lapply(1:length(R),
                function(y) {

                  stats <- lapply(1:length(weights),
                                  function(z) {
                                   
                                    pop$bin <- cut(pop[,x], breaks = breaks, labels = FALSE)
                                    
                                    samp <- pop[pop[R[y]]==1,]

                                    samp$weights = weights[[z]][[y]]

                                    weightedFreq <- lapply(unique(pop$bin),
                                                           function(x) {
                                                             data.frame(weighted_sample = sum(samp$weights[samp$bin==x]) / sum(samp$weights),
                                                                        sample = nrow(samp[samp$bin== x,]) / nrow(samp),
                                                                        population = nrow(pop[pop$bin == x,]) / nrow(pop),
                                                                        bin = x,
                                                                        var = z,
                                                                        id = RNames[y],
                                                                        weightType = WNames[z])
                                                           })
                                    
                                    weightedFreq <- do.call("rbind", weightedFreq)
                                    
                                    melt(weightedFreq, id = c("bin", "var", "id", "weightType"))
                                    
                                  })
                  
                  if (length(weights) > 1 | length(R) > 1) stats <- do.call("rbind", stats)
                  
                })

    
    if (length(weights) > 1 | length(R) > 1) dat <- do.call("rbind", dat)
    
    if (addVarByRelFreq == TRUE) {
      
      for (i in 1:length(R)) {
        
        dfWSamp <- varByRelFreq[[i]]
        
        dfSamp <- dat[dat$weightType == WNames[2] & dat$variable == "sample" & dat$id == dfWSamp$id,]
        
        dfSamp$weightType <- dfWSamp$weightType
        
        dfPop <- dat[dat$weightType == WNames[2] & dat$variable == "population" & dat$id == dfWSamp$id,]
        
        dfPop$weightType <- dfWSamp$weightType
        
        dat <- rbind(dat, dfWSamp, dfSamp, dfPop)

      }
      
      
    } 

    p <- ggplot(data=dat,aes(y = value, x = bin, colour = variable)) +
                geom_line() +
                theme_linedraw() +
                labs(colour = "",
                x = "",
                y = "Relative frequency")

    if (length(weights) > 1 | length(R) > 1) p <- p + facet_grid(id~weightType, 
                                                           scales = "free_y")
    
    return(list(plot = p, data = dat))
    
}

p_elev <- relFreqPlot(pop = pop,
            x = c("UKelv"),
            R = c("sampled_units_1987.1999", "sampled_units_2010.2019"),
            RNames = c("Period_1", "Period_2"),
            weights = list(list(p1 = 1/calib_design_p1$prob,
                                      p2 = 1/calib_design_p2$prob),
                           list(p1 = 1/ps_design_p1$prob,
                                      p2 = 1/ps_design_p2$prob),
                           list(p1 = 1/weighted_design_p1$prob,
                                      p2 = 1/weighted_design_p2$prob)),
            WNames = c("Superpopulation
model", "Poststratification", "Quasi-
randomisation"),
            breaks = 50,
            addVarByRelFreq = TRUE,
            varByRelFreq = list(relF_p1_elev,relF_p2_elev))

p_elev$plot

p_road <- relFreqPlot(pop = pop,
                      x = c("road_length_299_neighbours"),
                      R = c("sampled_units_1987.1999", "sampled_units_2010.2019"),
                      RNames = c("Period_1", "Period_2"),
                      weights = list(list(p1 = 1/calib_design_p1$prob,
                                          p2 = 1/calib_design_p2$prob),
                                     list(p1 = 1/ps_design_p1$prob,
                                          p2 = 1/ps_design_p2$prob),
                                     list(p1 = 1/weighted_design_p1$prob,
                                          p2 = 1/weighted_design_p2$prob)),
                      WNames = c("Superpopulation
model", "Poststratification", "Quasi-
randomisation"),
                      breaks = 50,
                      addVarByRelFreq = TRUE,
                      varByRelFreq = list(relF_p1_road,relF_p2_road))

p_road$plot
```

A visual comparison is fine, but it is better to do it formally. I created another function, auxImprovement, that assesses the deviations of the sample and weighted samples' relative frequency distributions from those in the population. The test statistic is the mean absolute error across all bins in the frequency distributions. 
```{r}
auxImprovement <- function(dat, period, estimator) {
  
  samp <- dat$data$value[dat$data$variable=="sample" & dat$data$id == period & dat$data$weightType == estimator]
  
  est <- dat$data$value[dat$data$variable=="weighted_sample" & dat$data$id == period & dat$data$weightType == estimator]
  
  pop <- dat$data$value[dat$data$variable=="population" & dat$data$id == period & dat$data$weightType == estimator]
  
  data.frame(mae_samp = mean(abs(pop-samp)),
             mae_est = mean(abs(pop-est)))

}

## road length in period one

auxImprovement(dat = p_road,
               period = "Period_1",
               estimator = "Subsample")

## road length in period two

auxImprovement(dat = p_road,
               period = "Period_1",
               estimator = "Subsample")

## elevation in period one

auxImprovement(dat = p_elev,
               period = "Period_1",
               estimator = "Subsample")

## elevation in period two

auxImprovement(dat = p_elev,
               period = "Period_2",
               estimator = "Subsample")
```
Note that we have not looked at the distributions of three of the five auxiliary variables or of the weighted samples produced by MRP. The shapes of the distributions of the other auxiliaries make it difficult to assess the effects of weighting. For our implementation of MRP, it is not clear how to obtain grid-square-level weights or relative frequencies. 
